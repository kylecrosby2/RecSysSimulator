x.shape
torch.Size([1, 1024, 768])
input_ids have 23 tokens
we and pe write: before add_cap to get xpe: latency in us: energy in uJ:
0.16128000000000003 0.00252626112
we+pe = xpe: output: latency in us, energy in uJ
0.7344 0.45613055999999996


Operation0: Embedding Layer latency: 0.89568 us, and Energy: 0.45865682111999995 uJ
w_q.shape
torch.Size([768, 768])
fet_mac_lat: 1.1 fet_mac_en: 74
debug fet_mac: xbar_count: 1152.0 wl_count: 1024 lat_operation_count: 1024 en_operation_count: 1179648.0 fet_mac_lat: 1.1 fet_mac_en: 74
fet_mac_lat: 1.1 fet_mac_en: 74
debug fet_mac: xbar_count: 1152.0 wl_count: 1024 lat_operation_count: 1024 en_operation_count: 1179648.0 fet_mac_lat: 1.1 fet_mac_en: 74
fet_mac_lat: 1.1 fet_mac_en: 74
debug fet_mac: xbar_count: 1152.0 wl_count: 1024 lat_operation_count: 1024 en_operation_count: 1179648.0 fet_mac_lat: 1.1 fet_mac_en: 74
lat,en: fet mac output compute q,k,v
4.274880000000001 262.34051282112
Operation1: Latency and energy from MUL of X and W_Q: 1.1264000000000003 us and 87.293952 uJ
query.shape
torch.Size([1, 1024, 768])
query.shape
torch.Size([1, 12, 1024, 64])
lat,en: after cap_bl_to_sa_wr_rm: q,k,v write
4.370880000000001 262.35747026111994
Operation2: cap_write q,k,v: Latency = 0.032 us and Energy = 0.016957439999967815 uJ
cap_mac_lat: 1.1 cap_mac_en: 71.07
debug cap_mac: xbar_count: 128.0 wl_count: 1024 lat_operation_count: 1024 en_operation_count: 1572864.0 cap_mac_lat: 1.1 cap_mac_en: 71.07
lat,en: after qkt mac computation
5.401312000000002 374.14091474111996
Operation3: cap mac QK_T: Latency = 1.1264000000000005 us and Energy = 111.78344448000001 uJ
attention_scores.shape
torch.Size([1, 12, 1024, 1024])
qk_t write output: lat,en in us and uJ
5.433312000000002 374.23135442112
Operation Softmax Layerwise: Softmax lat: 2.47296 us en: 1.904014786559999uJ
Operation Softmax Layerwise: cap_add lat: 0.44159999999999944 us en: 7.361003519999981uJ
Operation Softmax Layerwise: scaling lat: 0.0011000000000003637 us en: 1.746616319999993uJ
softmax output: lat,en in us and uJ
8.348972000000002 385.24298904767994


Operation4: Softmax and writing QK_T together:  Latency = 2.94766 us and Energy = 11.10207430655998 uJ
Operation softmax: Latency = 2.91566 us and Energy = 11.011634626559973
attention_scores.shape
torch.Size([1, 12, 1024, 1024])
cap_mac_lat: 1.1 cap_mac_en: 71.07
debug cap_mac: xbar_count: 128.0 wl_count: 1024 lat_operation_count: 1024 en_operation_count: 1572864.0 cap_mac_lat: 1.1 cap_mac_en: 71.07
qkt*v output: lat,en in us and uJ
9.475372000000002 497.0264335276799
Operation5: CAP-MAC Softmax output*V computation:  Latency = 1.1263999999999996 us and Energy = 111.78344447999996 uJ


attention_matrix.shape
torch.Size([1, 12, 1024, 64])
Operation5.5: lat_only_write_mha = 0.032 us and en_only_write_mha = 0.005652480000019074 uJ

after writing soft(qkt)*v in us and uJ
9.507372000000002 497.03208600767994


fet_mac_lat: 1.1 fet_mac_en: 74
debug fet_mac: xbar_count: 1152.0 wl_count: 1024 lat_operation_count: 1024 en_operation_count: 1179648.0 fet_mac_lat: 1.1 fet_mac_en: 74
MHA output lat,en results in us and uJ
10.633772 584.3260380076799
Operation6: FET MAC: Multiplication with W_O:  Latency = 1.1263999999999996 us and Energy = 87.293952 uJ


attention_matrix.shape
torch.Size([1, 1024, 768])
After writing MHA output to the FET: lat,en in us and uJ
11.913772000000002 584.3301544876799
add_dram:x with mha output:  lat,en in us and uJ
12.615532000000002 584.55330456768


Operation norm ADD: lat_only_norm_add: 0.7017600000000003 us and en_only_norm_add = 0.2231500800000429 uJ

layernorm output1 shape:
torch.Size([1, 1024, 768])
after writing the output of the x+mha_output in the fet: lat,en in us and uJ
12.615532000000002 584.55330456768


ff1 layer output shape:
torch.Size([1, 1024, 3072])
fet_mac_lat: 1.1 fet_mac_en: 74
batch: 1, seq_len: 1024, ff_hidden_dim: 3072, d_model: 768
debug fet_mac: xbar_count: 4608.0 wl_count: 1024 lat_operation_count: 1024 en_operation_count: 4718592.0 fet_mac_lat: 1.1 fet_mac_en: 74
ff1 mac output: lat,en in us and uJ
13.741932 933.72911256768
Operation7: FET MAC FF1:  Latency = 1.1263999999999996 us and Energy = 349.175808 uJ
ff1 mac output FET write: lat,en in us and uJ
15.021932000000001 933.74557848768


ff_relu shape:
torch.Size([1, 1024, 3072])
fet_mac_lat: 1.1 fet_mac_en: 74
batch: 1, seq_len: 1024, ff_hidden_dim: 3072, d_model: 768
debug fet_mac: xbar_count: 4608.0 wl_count: 1024 lat_operation_count: 1024 en_operation_count: 4718592.0 fet_mac_lat: 1.1 fet_mac_en: 74
FF2 mac operation output: lat,en in us and uJ
16.148332 1282.92138648768
Operation8: FET MAC FF2:  Latency = 1.1263999999999996 us and Energy = 349.175808 uJ
ff2 mac output FET write: lat,en in us and uJ
17.428332 1282.92550296768


ff_out.shape
torch.Size([1, 1024, 768])
torch.Size([1, 1024, 768])
Operation norm ADD2: latency = 0.7017599999999984 us and energy = 0.7017599999999984 uJ

Encoder layer output: add_dram x+ff2 : lat,en in us and uJ
18.130092 1283.14865304768
Encoder output write FET: lat,en in us and uJ
18.130092 1283.14865304768


lat_before_mha,lat_after_mha,lat_after_encoder in us and uJ
0.89568 9.507372000000002 18.130092
en_before_mha,en_after_mha,en_after_encoder in us and uJ
0.45865682111999995 497.03208600767994 1283.14865304768


end_to_end_latency is 0.207708624 ms and end_to_end_energy is 15.392738611539839 mJ
