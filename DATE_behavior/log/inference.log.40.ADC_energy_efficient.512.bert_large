x.shape
torch.Size([1, 512, 1024])
input_ids have 23 tokens
we and pe write: before add_cap to get xpe: latency in us: energy in uJ:
0.21503999999999998 0.00336834816
we+pe = xpe: output: latency in us, energy in uJ
0.7343999999999999 0.30408704


Operation0: Embedding Layer latency: 0.94944 us, and Energy: 0.30745538815999995 uJ
w_q.shape
torch.Size([1024, 1024])
fet_mac_lat: 10.1 fet_mac_en: 60.72
debug fet_mac: xbar_count: 2048.0 wl_count: 512 lat_operation_count: 512 en_operation_count: 1048576.0 fet_mac_lat: 10.1 fet_mac_en: 60.72
fet_mac_lat: 10.1 fet_mac_en: 60.72
debug fet_mac: xbar_count: 2048.0 wl_count: 512 lat_operation_count: 512 en_operation_count: 1048576.0 fet_mac_lat: 10.1 fet_mac_en: 60.72
fet_mac_lat: 10.1 fet_mac_en: 60.72
debug fet_mac: xbar_count: 2048.0 wl_count: 512 lat_operation_count: 512 en_operation_count: 1048576.0 fet_mac_lat: 10.1 fet_mac_en: 60.72
lat,en: fet mac output compute q,k,v
16.46304 191.31605954815998
Operation1: Latency and energy from MUL of X and W_Q: 5.1712 us and 63.66953472 uJ
query.shape
torch.Size([1, 512, 1024])
query.shape
torch.Size([1, 16, 512, 64])
lat,en: after cap_bl_to_sa_wr_rm: q,k,v write
16.55904 191.32736450815997
Operation2: cap_write q,k,v: Latency = 0.032 us and Energy = 0.011304959999978543 uJ
cap_mac_lat: 10.1 cap_mac_en: 56.99
debug cap_mac: xbar_count: 64.0 wl_count: 512 lat_operation_count: 512 en_operation_count: 524288.0 cap_mac_lat: 10.1 cap_mac_en: 56.99
lat,en: after qkt mac computation
21.634272 221.20653762815996
Operation3: cap mac QK_T: Latency = 5.171200000000001 us and Energy = 29.879173120000004 uJ
attention_scores.shape
torch.Size([1, 16, 512, 512])
qk_t write output: lat,en in us and uJ
21.666272 221.23668418815998
Operation Softmax Layerwise: Softmax lat: 0.8243199999999997 us en: 0.6346715955199898uJ
Operation Softmax Layerwise: cap_add lat: 0.22079999999999927 us en: 2.4536678400000036uJ
Operation Softmax Layerwise: scaling lat: 0.010099999999998544 us en: 0.4668620800000131uJ
softmax output: lat,en in us and uJ
22.721491999999998 224.79188570367998


Operation4: Softmax and writing QK_T together:  Latency = 1.0872199999999976 us and Energy = 3.5853480755200087 uJ
Operation softmax: Latency = 1.0552199999999976 us and Energy = 3.5552015155200065
attention_scores.shape
torch.Size([1, 16, 512, 512])
cap_mac_lat: 10.1 cap_mac_en: 56.99
debug cap_mac: xbar_count: 64.0 wl_count: 512 lat_operation_count: 512 en_operation_count: 524288.0 cap_mac_lat: 10.1 cap_mac_en: 56.99
qkt*v output: lat,en in us and uJ
27.892692 254.67105882367997
Operation5: CAP-MAC Softmax output*V computation:  Latency = 5.171200000000001 us and Energy = 29.879173120000004 uJ


attention_matrix.shape
torch.Size([1, 16, 512, 64])
Operation5.5: lat_only_write_mha = 0.032 us and en_only_write_mha = 0.0037683199999928477 uJ

after writing soft(qkt)*v in us and uJ
27.924692 254.67482714367998


fet_mac_lat: 10.1 fet_mac_en: 60.72
debug fet_mac: xbar_count: 2048.0 wl_count: 512 lat_operation_count: 512 en_operation_count: 1048576.0 fet_mac_lat: 10.1 fet_mac_en: 60.72
MHA output lat,en results in us and uJ
33.095892 318.34436186368
Operation6: FET MAC: Multiplication with W_O:  Latency = 5.171200000000001 us and Energy = 63.66953472000003 uJ


attention_matrix.shape
torch.Size([1, 512, 1024])
After writing MHA output to the FET: lat,en in us and uJ
34.375892 318.34710618368
add_dram:x with mha output:  lat,en in us and uJ
35.077652 318.49587290368004


Operation norm ADD: lat_only_norm_add: 0.701760000000002 us and en_only_norm_add = 0.1487667200000286 uJ

layernorm output1 shape:
torch.Size([1, 512, 1024])
after writing the output of the x+mha_output in the fet: lat,en in us and uJ
35.077652 318.49587290368004


ff1 layer output shape:
torch.Size([1, 512, 4096])
fet_mac_lat: 10.1 fet_mac_en: 60.72
batch: 1, seq_len: 512, ff_hidden_dim: 4096, d_model: 1024
debug fet_mac: xbar_count: 8192.0 wl_count: 512 lat_operation_count: 512 en_operation_count: 4194304.0 fet_mac_lat: 10.1 fet_mac_en: 60.72
ff1 mac output: lat,en in us and uJ
40.248852 573.1740117836799
Operation7: FET MAC FF1:  Latency = 5.171199999999997 us and Energy = 254.67813887999995 uJ
ff1 mac output FET write: lat,en in us and uJ
41.528852 573.18498906368


ff_relu shape:
torch.Size([1, 512, 4096])
fet_mac_lat: 10.1 fet_mac_en: 60.72
batch: 1, seq_len: 512, ff_hidden_dim: 4096, d_model: 1024
debug fet_mac: xbar_count: 8192.0 wl_count: 512 lat_operation_count: 512 en_operation_count: 4194304.0 fet_mac_lat: 10.1 fet_mac_en: 60.72
FF2 mac operation output: lat,en in us and uJ
46.700052 827.86312794368
Operation8: FET MAC FF2:  Latency = 5.171199999999997 us and Energy = 254.67813888 uJ
ff2 mac output FET write: lat,en in us and uJ
47.98005199999999 827.86587226368


ff_out.shape
torch.Size([1, 512, 1024])
torch.Size([1, 512, 1024])
Operation norm ADD2: latency = 0.701760000000002 us and energy = 0.701760000000002 uJ

Encoder layer output: add_dram x+ff2 : lat,en in us and uJ
48.681812 828.01463898368
Encoder output write FET: lat,en in us and uJ
48.681812 828.01463898368


lat_before_mha,lat_after_mha,lat_after_encoder in us and uJ
0.94944 27.924692 48.681812
en_before_mha,en_after_mha,en_after_encoder in us and uJ
0.30745538815999995 254.67482714367998 828.01463898368


end_to_end_latency is 1.1465263679999997 ms and end_to_end_energy is 19.86527986168064 mJ
