DatasetDict({
    train: Dataset({
        features: ['id', 'translation'],
        num_rows: 32332
    })
})
{'id': '0', 'translation': {'en': 'Source: Project Gutenberg', 'it': 'Source: www.liberliber.it/Audiobook available here'}}
{'en': 'Source: Project Gutenberg', 'it': 'Source: www.liberliber.it/Audiobook available here'}
Source: Project Gutenberg
<class 'dict'>
{'en': 'Source: Project Gutenberg', 'it': 'Source: www.liberliber.it/Audiobook available here'}
tokenized_datasets ---->>
{'input_ids': tensor([[  101,  3120,  1024,  ...,     0,     0,     0],
        [  101,  4869, 26975,  ...,     0,     0,     0],
        [  101,  5904, 22953,  ...,     0,     0,     0],
        ...,
        [  101,  1996,  2056,  ...,     0,     0,     0],
        [  101,  2033,  1010,  ...,     0,     0,     0],
        [  101,  1000,  2054,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]])}
torch.Size([10, 512])
torch.Size([10, 512])
torch.Size([10, 512])
tensor(29566)
x.shape
torch.Size([10, 512, 768])
input embedding write: 
tensor([[[ 0.7272,  0.0000, -0.0000,  ..., -1.8940, -0.3385,  2.6001],
         [-0.1350,  0.7221,  1.4042,  ..., -0.0000,  1.3568, -0.8703],
         [ 0.5351,  1.1853, -0.7607,  ..., -0.2489, -0.3532,  0.8055],
         ...,
         [-0.0000, -2.5291,  0.1284,  ...,  0.2489,  1.0548, -0.7922],
         [-1.3798, -2.5291,  0.1284,  ...,  0.2489,  0.0000, -0.7922],
         [-1.3798, -2.5291,  0.1284,  ...,  0.2489,  1.0548, -0.7922]],

        [[ 0.7272,  0.2921, -0.2766,  ..., -1.8940, -0.3385,  2.6001],
         [ 1.0674, -1.6644,  1.5446,  ...,  1.1971, -0.6354, -2.1828],
         [-0.1544, -1.4067, -2.3515,  ..., -1.3034, -0.5778,  1.4646],
         ...,
         [-1.3798, -2.5291,  0.1284,  ...,  0.2489,  1.0548, -0.7922],
         [-1.3798, -2.5291,  0.1284,  ...,  0.2489,  1.0548, -0.7922],
         [-0.0000, -2.5291,  0.1284,  ...,  0.2489,  1.0548, -0.7922]],

        [[ 0.7272,  0.2921, -0.2766,  ..., -1.8940, -0.3385,  2.6001],
         [ 0.7523, -1.3553,  1.4343,  ...,  0.1013,  0.7884,  0.8656],
         [ 0.5632, -2.6442, -0.0000,  ..., -0.7770, -0.1980, -1.0159],
         ...,
         [-1.3798, -2.5291,  0.1284,  ...,  0.2489,  1.0548, -0.0000],
         [-1.3798, -0.0000,  0.1284,  ...,  0.2489,  1.0548, -0.7922],
         [-1.3798, -2.5291,  0.1284,  ...,  0.2489,  1.0548, -0.7922]],

        ...,

        [[ 0.7272,  0.2921, -0.2766,  ..., -1.8940, -0.0000,  2.6001],
         [ 1.4947,  0.8046, -1.6296,  ...,  1.9031, -0.4087,  0.3652],
         [ 0.5634, -0.5718,  0.8497,  ..., -0.7307,  1.9289,  2.4579],
         ...,
         [-1.3798, -2.5291,  0.1284,  ...,  0.2489,  1.0548, -0.7922],
         [-1.3798, -2.5291,  0.1284,  ...,  0.2489,  1.0548, -0.7922],
         [-1.3798, -2.5291,  0.1284,  ...,  0.2489,  1.0548, -0.7922]],

        [[ 0.0000,  0.0000, -0.2766,  ..., -1.8940, -0.3385,  0.0000],
         [ 0.8929, -0.4790, -0.0000,  ..., -1.8523,  0.4573, -0.5934],
         [-1.6338, -0.0000, -1.5200,  ..., -0.2250, -0.4189,  0.8023],
         ...,
         [-1.3798, -2.5291,  0.1284,  ...,  0.2489,  1.0548, -0.7922],
         [-1.3798, -2.5291,  0.1284,  ...,  0.2489,  1.0548, -0.7922],
         [-1.3798, -2.5291,  0.1284,  ...,  0.2489,  1.0548, -0.7922]],

        [[ 0.7272,  0.2921, -0.0000,  ..., -1.8940, -0.3385,  2.6001],
         [-0.0000,  0.8358,  0.7474,  ...,  1.1898, -1.1562,  0.6219],
         [-0.9249,  0.7430,  0.4103,  ..., -2.0733,  0.4234, -0.5446],
         ...,
         [-0.0000, -2.5291,  0.1284,  ...,  0.2489,  1.0548, -0.7922],
         [-1.3798, -2.5291,  0.1284,  ...,  0.2489,  1.0548, -0.7922],
         [-0.0000, -2.5291,  0.1284,  ...,  0.2489,  1.0548, -0.7922]]],
       grad_fn=<MulBackward0>)
input_ids have 872 tokens
word embedding token fetch output: latency=0, energy=0
lat,en: cap_in_write output after writing x and pe
491520 0
lat,en: add_cap_to_fet: output after adding x and pe and writing xpe
491520 0
lat,en: after writing xpe in FET
499968.0 466329.6
w_q.weight
Parameter containing:
tensor([[ 8.9816e-03,  3.4766e-02, -7.9928e-05,  ..., -1.4713e-02,
          1.6254e-02,  2.7831e-02],
        [-3.4920e-02, -4.2908e-03, -4.0228e-03,  ...,  1.4757e-02,
         -2.6859e-02,  3.2799e-02],
        [ 2.4558e-03,  2.6383e-02,  2.3380e-02,  ..., -1.2082e-03,
         -6.3245e-03, -2.1843e-02],
        ...,
        [ 1.5830e-02, -2.7638e-02, -1.8673e-02,  ..., -1.9694e-02,
          3.4811e-02,  1.0277e-02],
        [-2.0190e-03, -2.6893e-02,  3.6041e-02,  ..., -2.3694e-02,
         -2.2242e-02, -1.0425e-02],
        [ 6.1582e-03,  2.5420e-02,  3.8732e-03,  ...,  3.2105e-02,
         -3.5157e-02,  1.9497e-02]], requires_grad=True)
w_q.shape
torch.Size([768, 768])
lat,en: fet mac output compute q,k,v
18686124288.000004 1031446928793.6
query.shape
torch.Size([10, 512, 768])
query.shape
torch.Size([10, 12, 512, 64])
lat,en: after cap_bl_to_sa_wr_rm: q,k,v write
18686149632.000004 1031448566246.3999
lat,en: after qkt mac computation
22838510592.000004 1299728832998.4
attention_scores.shape
torch.Size([10, 12, 512, 512])
qk_t write output: lat,en
22838578176.000004 1299733199539.2
softmax output: lat,en
22838578176.000004 1299733199539.2
attention_scores.shape
torch.Size([10, 12, 512, 512])
qkt*v output: lat,en
56057465856.00001 3445975333555.1997
attention_matrix.shape
torch.Size([10, 12, 512, 64])
after writing soft(qkt)*v
56057474304.00001 3445975879372.8
MHA output lat,en results
62286015744.00001 3789791366860.8
attention_matrix.shape
torch.Size([10, 512, 768])
After writing MHA output to the FET: lat,en
62286024192.00001 3789791833190.4
add_dram: lat,en
62286024192.00001 3789791833190.4
layernorm output1 shape:
torch.Size([10, 512, 768])
after writing the output of the x+mha_output in the fet: lat,en
62286032640.00001 3789792299520.0
ff1 layer output shape:
torch.Size([10, 512, 3072])
ff1 mac output: lat,en
87200198400.00002 5165054249472.0
ff1 mac output FET write: lat,en
87200232192.00002 5165056114790.4
ff_relu shape:
torch.Size([10, 512, 3072])
FF2 mac operation output: lat,en
112114397952.00002 6540318064742.4
ff2 mac output FET write: lat,en
112114406400.00002 6540318531072.0
ff_out.shape
torch.Size([10, 512, 768])
torch.Size([10, 512, 768])
Encoder layer output: add_dram x+ff2 : lat,en
112114406400.00002 6540318531072.0
Encoder output write FET: lat,en
112114414848.00002 6540318997401.6
Encoder layer output shape:
torch.Size([10, 512, 768])
input_ids
tensor([[  101,  3120,  1024,  ...,     0,     0,     0],
        [  101,  4869, 26975,  ...,     0,     0,     0],
        [  101,  5904, 22953,  ...,     0,     0,     0],
        ...,
        [  101,  1996,  2056,  ...,     0,     0,     0],
        [  101,  2033,  1010,  ...,     0,     0,     0],
        [  101,  1000,  2054,  ...,     0,     0,     0]])
torch.Size([10, 512])
lat_before_mha,lat_after_mha,lat_after_encoder
499968.0 62286015744.00001 112114414848.00002
en_before_mha,en_after_mha,en_after_encoder
466329.6 3789791366860.8 6540318997401.6
end_to_end_latency is 660226804992.0001 and end_to_end_energy is 78483822839193.6
