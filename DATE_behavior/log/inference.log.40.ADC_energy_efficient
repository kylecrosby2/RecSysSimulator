x.shape
torch.Size([1, 1024, 1024])
input_ids have 23 tokens
we and pe write: before add_cap to get xpe: latency in us: energy in uJ:
0.21503999999999998 0.00336834816
we+pe = xpe: output: latency in us, energy in uJ
0.7343999999999999 0.60817408


Operation0: Embedding Layer latency: 0.94944 us, and Energy: 0.61154242816 uJ
w_q.shape
torch.Size([1024, 1024])
fet_mac_lat: 10.1 fet_mac_en: 60.72
debug fet_mac: xbar_count: 2048.0 wl_count: 1024 lat_operation_count: 1024 en_operation_count: 2097152.0 fet_mac_lat: 10.1 fet_mac_en: 60.72
fet_mac_lat: 10.1 fet_mac_en: 60.72
debug fet_mac: xbar_count: 2048.0 wl_count: 1024 lat_operation_count: 1024 en_operation_count: 2097152.0 fet_mac_lat: 10.1 fet_mac_en: 60.72
fet_mac_lat: 10.1 fet_mac_en: 60.72
debug fet_mac: xbar_count: 2048.0 wl_count: 1024 lat_operation_count: 1024 en_operation_count: 2097152.0 fet_mac_lat: 10.1 fet_mac_en: 60.72
lat,en: fet mac output compute q,k,v
31.97664 382.62875074816003
Operation1: Latency and energy from MUL of X and W_Q: 10.3424 us and 127.33906944 uJ
query.shape
torch.Size([1, 1024, 1024])
query.shape
torch.Size([1, 16, 1024, 64])
lat,en: after cap_bl_to_sa_wr_rm: q,k,v write
32.07264 382.65136066815995
Operation2: cap_write q,k,v: Latency = 0.032 us and Energy = 0.022609919999957085 uJ
cap_mac_lat: 10.1 cap_mac_en: 56.99
debug cap_mac: xbar_count: 128.0 wl_count: 1024 lat_operation_count: 1024 en_operation_count: 2097152.0 cap_mac_lat: 10.1 cap_mac_en: 56.99
lat,en: after qkt mac computation
42.319072 502.16805314815997
Operation3: cap mac QK_T: Latency = 10.342400000000001 us and Energy = 119.51669248000002 uJ
attention_scores.shape
torch.Size([1, 16, 1024, 1024])
qk_t write output: lat,en in us and uJ
42.351072 502.28863938816
Operation Softmax Layerwise: Softmax lat: 3.297279999999999 us en: 2.5386863820800185uJ
Operation Softmax Layerwise: cap_add lat: 0.44159999999999855 us en: 9.814671360000014uJ
Operation Softmax Layerwise: scaling lat: 0.010099999999998544 us en: 1.867448319999993uJ
softmax output: lat,en in us and uJ
46.100052 516.50944545024


Operation4: Softmax and writing QK_T together:  Latency = 3.780979999999996 us and Energy = 14.341392302080035 uJ
Operation softmax: Latency = 3.748979999999996 us and Energy = 14.220806062080026
attention_scores.shape
torch.Size([1, 16, 1024, 1024])
cap_mac_lat: 10.1 cap_mac_en: 56.99
debug cap_mac: xbar_count: 128.0 wl_count: 1024 lat_operation_count: 1024 en_operation_count: 2097152.0 cap_mac_lat: 10.1 cap_mac_en: 56.99
qkt*v output: lat,en in us and uJ
56.442451999999996 636.02613793024
Operation5: CAP-MAC Softmax output*V computation:  Latency = 10.342400000000001 us and Energy = 119.51669248000002 uJ


attention_matrix.shape
torch.Size([1, 16, 1024, 64])
Operation5.5: lat_only_write_mha = 0.032 us and en_only_write_mha = 0.007536639999985695 uJ

after writing soft(qkt)*v in us and uJ
56.474452 636.03367457024


fet_mac_lat: 10.1 fet_mac_en: 60.72
debug fet_mac: xbar_count: 2048.0 wl_count: 1024 lat_operation_count: 1024 en_operation_count: 2097152.0 fet_mac_lat: 10.1 fet_mac_en: 60.72
MHA output lat,en results in us and uJ
66.816852 763.3727440102401
Operation6: FET MAC: Multiplication with W_O:  Latency = 10.342400000000001 us and Energy = 127.33906944000006 uJ


attention_matrix.shape
torch.Size([1, 1024, 1024])
After writing MHA output to the FET: lat,en in us and uJ
68.096852 763.3782326502401
add_dram:x with mha output:  lat,en in us and uJ
68.79861199999999 763.6757660902401


Operation norm ADD: lat_only_norm_add: 0.7017599999999947 us and en_only_norm_add = 0.2975334400000572 uJ

layernorm output1 shape:
torch.Size([1, 1024, 1024])
after writing the output of the x+mha_output in the fet: lat,en in us and uJ
68.79861199999999 763.6757660902401


ff1 layer output shape:
torch.Size([1, 1024, 4096])
fet_mac_lat: 10.1 fet_mac_en: 60.72
batch: 1, seq_len: 1024, ff_hidden_dim: 4096, d_model: 1024
debug fet_mac: xbar_count: 8192.0 wl_count: 1024 lat_operation_count: 1024 en_operation_count: 8388608.0 fet_mac_lat: 10.1 fet_mac_en: 60.72
ff1 mac output: lat,en in us and uJ
79.14101199999999 1273.0320438502401
Operation7: FET MAC FF1:  Latency = 10.342399999999994 us and Energy = 509.3562777600001 uJ
ff1 mac output FET write: lat,en in us and uJ
80.42101199999999 1273.0539984102402


ff_relu shape:
torch.Size([1, 1024, 4096])
fet_mac_lat: 10.1 fet_mac_en: 60.72
batch: 1, seq_len: 1024, ff_hidden_dim: 4096, d_model: 1024
debug fet_mac: xbar_count: 8192.0 wl_count: 1024 lat_operation_count: 1024 en_operation_count: 8388608.0 fet_mac_lat: 10.1 fet_mac_en: 60.72
FF2 mac operation output: lat,en in us and uJ
90.76341199999999 1782.4102761702402
Operation8: FET MAC FF2:  Latency = 10.342399999999994 us and Energy = 509.35627776 uJ
ff2 mac output FET write: lat,en in us and uJ
92.04341199999998 1782.4157648102403


ff_out.shape
torch.Size([1, 1024, 1024])
torch.Size([1, 1024, 1024])
Operation norm ADD2: latency = 0.7017599999999947 us and energy = 0.7017599999999947 uJ

Encoder layer output: add_dram x+ff2 : lat,en in us and uJ
92.74517199999998 1782.7132982502403
Encoder output write FET: lat,en in us and uJ
92.74517199999998 1782.7132982502403


lat_before_mha,lat_after_mha,lat_after_encoder in us and uJ
0.94944 56.474452 92.74517199999998
en_before_mha,en_after_mha,en_after_encoder in us and uJ
0.61154242816 636.03367457024 1782.7132982502403


end_to_end_latency is 2.2040470079999994 ms and end_to_end_energy is 42.77105368215809 mJ
