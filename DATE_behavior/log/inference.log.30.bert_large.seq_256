x.shape
torch.Size([1, 256, 1024])
input_ids have 23 tokens
we and pe write: before add_cap to get xpe: latency in us: energy in uJ:
0.21503999999999998 0.00336834816
we+pe = xpe: output: latency in us, energy in uJ
0.7343999999999999 0.15204352


Operation0: Embedding Layer latency: 0.94944 us, and Energy: 0.15541186816 uJ
w_q.shape
torch.Size([1024, 1024])
fet_mac_lat: 1.1 fet_mac_en: 74
debug fet_mac: xbar_count: 2048.0 wl_count: 256 lat_operation_count: 256 en_operation_count: 524288.0 fet_mac_lat: 1.1 fet_mac_en: 74
fet_mac_lat: 1.1 fet_mac_en: 74
debug fet_mac: xbar_count: 2048.0 wl_count: 256 lat_operation_count: 256 en_operation_count: 524288.0 fet_mac_lat: 1.1 fet_mac_en: 74
fet_mac_lat: 1.1 fet_mac_en: 74
debug fet_mac: xbar_count: 2048.0 wl_count: 256 lat_operation_count: 256 en_operation_count: 524288.0 fet_mac_lat: 1.1 fet_mac_en: 74
lat,en: fet mac output compute q,k,v
1.7942399999999998 116.54734786816
Operation1: Latency and energy from MUL of X and W_Q: 0.28159999999999996 us and 38.797312000000005 uJ
query.shape
torch.Size([1, 256, 1024])
query.shape
torch.Size([1, 16, 256, 64])
lat,en: after cap_bl_to_sa_wr_rm: q,k,v write
1.8902399999999997 116.55300034816
Operation2: cap_write q,k,v: Latency = 0.032 us and Energy = 0.005652479999989271 uJ
cap_mac_lat: 1.1 cap_mac_en: 71.07
debug cap_mac: xbar_count: 32.0 wl_count: 256 lat_operation_count: 256 en_operation_count: 131072.0 cap_mac_lat: 1.1 cap_mac_en: 71.07
lat,en: after qkt mac computation
2.075872 125.86828738815998
Operation3: cap mac QK_T: Latency = 0.28160000000000013 us and Energy = 9.315287039999992 uJ
attention_scores.shape
torch.Size([1, 16, 256, 256])
qk_t write output: lat,en in us and uJ
2.107872 125.87582402816
Operation Softmax Layerwise: Softmax lat: 0.20607999999999993 us en: 0.15866789888000488uJ
Operation Softmax Layerwise: cap_add lat: 0.1104000000000001 us en: 0.6134169599999935uJ
Operation Softmax Layerwise: scaling lat: 0.001099999999999909 us en: 0.1455513599999994uJ
softmax output: lat,en in us and uJ
2.425452 126.79346024703999


Operation4: Softmax and writing QK_T together:  Latency = 0.34957999999999995 us and Energy = 0.9251728588799983 uJ
Operation softmax: Latency = 0.3175799999999999 us and Energy = 0.9176362188799977
attention_scores.shape
torch.Size([1, 16, 256, 256])
cap_mac_lat: 1.1 cap_mac_en: 71.07
debug cap_mac: xbar_count: 32.0 wl_count: 256 lat_operation_count: 256 en_operation_count: 131072.0 cap_mac_lat: 1.1 cap_mac_en: 71.07
qkt*v output: lat,en in us and uJ
2.7070519999999996 136.10874728704
Operation5: CAP-MAC Softmax output*V computation:  Latency = 0.2815999999999999 us and Energy = 9.315287040000007 uJ


attention_matrix.shape
torch.Size([1, 16, 256, 64])
Operation5.5: lat_only_write_mha = 0.032 us and en_only_write_mha = 0.0018841599999964238 uJ

after writing soft(qkt)*v in us and uJ
2.7390519999999996 136.11063144704


fet_mac_lat: 1.1 fet_mac_en: 74
debug fet_mac: xbar_count: 2048.0 wl_count: 256 lat_operation_count: 256 en_operation_count: 524288.0 fet_mac_lat: 1.1 fet_mac_en: 74
MHA output lat,en results in us and uJ
3.0206519999999997 174.90794344704
Operation6: FET MAC: Multiplication with W_O:  Latency = 0.2815999999999999 us and Energy = 38.797312 uJ


attention_matrix.shape
torch.Size([1, 256, 1024])
After writing MHA output to the FET: lat,en in us and uJ
4.300652 174.90931560703999
add_dram:x with mha output:  lat,en in us and uJ
5.0024120000000005 174.98369896704


Operation norm ADD: lat_only_norm_add: 0.7017600000000003 us and en_only_norm_add = 0.0743833600000143 uJ

layernorm output1 shape:
torch.Size([1, 256, 1024])
after writing the output of the x+mha_output in the fet: lat,en in us and uJ
5.0024120000000005 174.98369896704


ff1 layer output shape:
torch.Size([1, 256, 4096])
fet_mac_lat: 1.1 fet_mac_en: 74
batch: 1, seq_len: 256, ff_hidden_dim: 4096, d_model: 1024
debug fet_mac: xbar_count: 8192.0 wl_count: 256 lat_operation_count: 256 en_operation_count: 2097152.0 fet_mac_lat: 1.1 fet_mac_en: 74
ff1 mac output: lat,en in us and uJ
5.284012000000001 330.17294696704
Operation7: FET MAC FF1:  Latency = 0.28160000000000035 us and Energy = 155.189248 uJ
ff1 mac output FET write: lat,en in us and uJ
6.564012000000001 330.17843560704


ff_relu shape:
torch.Size([1, 256, 4096])
fet_mac_lat: 1.1 fet_mac_en: 74
batch: 1, seq_len: 256, ff_hidden_dim: 4096, d_model: 1024
debug fet_mac: xbar_count: 8192.0 wl_count: 256 lat_operation_count: 256 en_operation_count: 2097152.0 fet_mac_lat: 1.1 fet_mac_en: 74
FF2 mac operation output: lat,en in us and uJ
6.845612000000001 485.36768360703996
Operation8: FET MAC FF2:  Latency = 0.28160000000000035 us and Energy = 155.189248 uJ
ff2 mac output FET write: lat,en in us and uJ
8.125612 485.36905576704004


ff_out.shape
torch.Size([1, 256, 1024])
torch.Size([1, 256, 1024])
Operation norm ADD2: latency = 0.7017600000000003 us and energy = 0.7017600000000003 uJ

Encoder layer output: add_dram x+ff2 : lat,en in us and uJ
8.827372 485.44343912704005
Encoder output write FET: lat,en in us and uJ
8.827372 485.44343912704005


lat_before_mha,lat_after_mha,lat_after_encoder in us and uJ
0.94944 2.7390519999999996 8.827372
en_before_mha,en_after_mha,en_after_encoder in us and uJ
0.15541186816 136.11063144704 485.44343912704005


end_to_end_latency is 0.19001980800000004 ms and end_to_end_energy is 11.64706806608128 mJ
