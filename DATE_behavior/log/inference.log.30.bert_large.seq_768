x.shape
torch.Size([1, 768, 1024])
input_ids have 23 tokens
we and pe write: before add_cap to get xpe: latency in us: energy in uJ:
0.21503999999999998 0.00336834816
we+pe = xpe: output: latency in us, energy in uJ
0.7343999999999999 0.45613055999999996


Operation0: Embedding Layer latency: 0.94944 us, and Energy: 0.45949890815999994 uJ
w_q.shape
torch.Size([1024, 1024])
fet_mac_lat: 1.1 fet_mac_en: 74
debug fet_mac: xbar_count: 2048.0 wl_count: 768 lat_operation_count: 768 en_operation_count: 1572864.0 fet_mac_lat: 1.1 fet_mac_en: 74
fet_mac_lat: 1.1 fet_mac_en: 74
debug fet_mac: xbar_count: 2048.0 wl_count: 768 lat_operation_count: 768 en_operation_count: 1572864.0 fet_mac_lat: 1.1 fet_mac_en: 74
fet_mac_lat: 1.1 fet_mac_en: 74
debug fet_mac: xbar_count: 2048.0 wl_count: 768 lat_operation_count: 768 en_operation_count: 1572864.0 fet_mac_lat: 1.1 fet_mac_en: 74
lat,en: fet mac output compute q,k,v
3.4838400000000003 349.63530690816
Operation1: Latency and energy from MUL of X and W_Q: 0.8448 us and 116.391936 uJ
query.shape
torch.Size([1, 768, 1024])
query.shape
torch.Size([1, 16, 768, 64])
lat,en: after cap_bl_to_sa_wr_rm: q,k,v write
3.5798400000000004 349.65226434816003
Operation2: cap_write q,k,v: Latency = 0.032 us and Energy = 0.01695744000005722 uJ
cap_mac_lat: 1.1 cap_mac_en: 71.07
debug cap_mac: xbar_count: 96.0 wl_count: 768 lat_operation_count: 768 en_operation_count: 1179648.0 cap_mac_lat: 1.1 cap_mac_en: 71.07
lat,en: after qkt mac computation
4.328672 433.48984770816
Operation3: cap mac QK_T: Latency = 0.8448000000000002 us and Energy = 83.83758336000001 uJ
attention_scores.shape
torch.Size([1, 16, 768, 768])
qk_t write output: lat,en in us and uJ
4.360672 433.55767746816
Operation Softmax Layerwise: Softmax lat: 1.8547200000000001 us en: 1.4280110899199843uJ
Operation Softmax Layerwise: cap_add lat: 0.33119999999999983 us en: 5.520752639999985uJ
Operation Softmax Layerwise: scaling lat: 0.0011000000000003637 us en: 1.3099622400000095uJ
softmax output: lat,en in us and uJ
6.5476920000000005 441.81640343808


Operation4: Softmax and writing QK_T together:  Latency = 2.2190200000000004 us and Energy = 8.32655572991997 uJ
Operation softmax: Latency = 2.1870200000000004 us and Energy = 8.258725969919979
attention_scores.shape
torch.Size([1, 16, 768, 768])
cap_mac_lat: 1.1 cap_mac_en: 71.07
debug cap_mac: xbar_count: 96.0 wl_count: 768 lat_operation_count: 768 en_operation_count: 1179648.0 cap_mac_lat: 1.1 cap_mac_en: 71.07
qkt*v output: lat,en in us and uJ
7.392492000000001 525.6539867980799
Operation5: CAP-MAC Softmax output*V computation:  Latency = 0.8448000000000002 us and Energy = 83.83758335999995 uJ


attention_matrix.shape
torch.Size([1, 16, 768, 64])
Operation5.5: lat_only_write_mha = 0.032 us and en_only_write_mha = 0.005652480000019074 uJ

after writing soft(qkt)*v in us and uJ
7.424492000000001 525.65963927808


fet_mac_lat: 1.1 fet_mac_en: 74
debug fet_mac: xbar_count: 2048.0 wl_count: 768 lat_operation_count: 768 en_operation_count: 1572864.0 fet_mac_lat: 1.1 fet_mac_en: 74
MHA output lat,en results in us and uJ
8.269292000000002 642.05157527808
Operation6: FET MAC: Multiplication with W_O:  Latency = 0.8448000000000002 us and Energy = 116.391936 uJ


attention_matrix.shape
torch.Size([1, 768, 1024])
After writing MHA output to the FET: lat,en in us and uJ
9.549292000000001 642.05569175808
add_dram:x with mha output:  lat,en in us and uJ
10.251052000000001 642.2788418380801


Operation norm ADD: lat_only_norm_add: 0.7017600000000003 us and en_only_norm_add = 0.2231500800000429 uJ

layernorm output1 shape:
torch.Size([1, 768, 1024])
after writing the output of the x+mha_output in the fet: lat,en in us and uJ
10.251052000000001 642.2788418380801


ff1 layer output shape:
torch.Size([1, 768, 4096])
fet_mac_lat: 1.1 fet_mac_en: 74
batch: 1, seq_len: 768, ff_hidden_dim: 4096, d_model: 1024
debug fet_mac: xbar_count: 8192.0 wl_count: 768 lat_operation_count: 768 en_operation_count: 6291456.0 fet_mac_lat: 1.1 fet_mac_en: 74
ff1 mac output: lat,en in us and uJ
11.095852 1107.8465858380798
Operation7: FET MAC FF1:  Latency = 0.8447999999999992 us and Energy = 465.5677439999999 uJ
ff1 mac output FET write: lat,en in us and uJ
12.375852 1107.86305175808


ff_relu shape:
torch.Size([1, 768, 4096])
fet_mac_lat: 1.1 fet_mac_en: 74
batch: 1, seq_len: 768, ff_hidden_dim: 4096, d_model: 1024
debug fet_mac: xbar_count: 8192.0 wl_count: 768 lat_operation_count: 768 en_operation_count: 6291456.0 fet_mac_lat: 1.1 fet_mac_en: 74
FF2 mac operation output: lat,en in us and uJ
13.220652 1573.43079575808
Operation8: FET MAC FF2:  Latency = 0.8447999999999992 us and Energy = 465.567744 uJ
ff2 mac output FET write: lat,en in us and uJ
14.500652 1573.43491223808


ff_out.shape
torch.Size([1, 768, 1024])
torch.Size([1, 768, 1024])
Operation norm ADD2: latency = 0.7017600000000003 us and energy = 0.7017600000000003 uJ

Encoder layer output: add_dram x+ff2 : lat,en in us and uJ
15.202412 1573.65806231808
Encoder output write FET: lat,en in us and uJ
15.202412 1573.65806231808


lat_before_mha,lat_after_mha,lat_after_encoder in us and uJ
0.94944 7.424492000000001 15.202412
en_before_mha,en_after_mha,en_after_encoder in us and uJ
0.45949890815999994 525.65963927808 1573.65806231808


end_to_end_latency is 0.343020768 ms and end_to_end_energy is 37.75722502074624 mJ
