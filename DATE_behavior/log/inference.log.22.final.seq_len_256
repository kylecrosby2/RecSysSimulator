x.shape
torch.Size([1, 256, 768])
input_ids have 23 tokens
we and pe write: before add_cap to get xpe: latency in us: energy in uJ:
0.16128000000000003 0.00252626112
we+pe = xpe: output: latency in us, energy in uJ
0.7344 0.11403263999999999


Operation0: Embedding Layer latency: 0.89568 us, and Energy: 0.11655890111999997 uJ
w_q.shape
torch.Size([768, 768])
fet_mac_lat: 1.1 fet_mac_en: 74
debug fet_mac: xbar_count: 1152.0 wl_count: 256 lat_operation_count: 256 en_operation_count: 294912.0 fet_mac_lat: 1.1 fet_mac_en: 74
fet_mac_lat: 1.1 fet_mac_en: 74
debug fet_mac: xbar_count: 1152.0 wl_count: 256 lat_operation_count: 256 en_operation_count: 294912.0 fet_mac_lat: 1.1 fet_mac_en: 74
fet_mac_lat: 1.1 fet_mac_en: 74
debug fet_mac: xbar_count: 1152.0 wl_count: 256 lat_operation_count: 256 en_operation_count: 294912.0 fet_mac_lat: 1.1 fet_mac_en: 74
lat,en: fet mac output compute q,k,v
1.74048 65.58702290112
Operation1: Latency and energy from MUL of X and W_Q: 0.28159999999999996 us and 21.823488 uJ
query.shape
torch.Size([1, 256, 768])
query.shape
torch.Size([1, 12, 256, 64])
lat,en: after cap_bl_to_sa_wr_rm: q,k,v write
1.8364800000000001 65.59126226112
Operation2: cap_write q,k,v: Latency = 0.032 us and Energy = 0.004239359999991954 uJ
cap_mac_lat: 1.1 cap_mac_en: 71.07
debug cap_mac: xbar_count: 32.0 wl_count: 256 lat_operation_count: 256 en_operation_count: 98304.0 cap_mac_lat: 1.1 cap_mac_en: 71.07
lat,en: after qkt mac computation
2.022112 72.57772754112
Operation3: cap mac QK_T: Latency = 0.28160000000000013 us and Energy = 6.986465280000001 uJ
attention_scores.shape
torch.Size([1, 12, 256, 256])
qk_t write output: lat,en in us and uJ
2.054112 72.58338002111999
Operation Softmax Layerwise: Softmax lat: 0.15455999999999995 us en: 0.11900092416000366uJ
Operation Softmax Layerwise: cap_add lat: 0.1104000000000001 us en: 0.4600627199999988uJ
Operation Softmax Layerwise: scaling lat: 0.001099999999999909 us en: 0.10916351999999582uJ
softmax output: lat,en in us and uJ
2.320172 73.27160718527999


Operation4: Softmax and writing QK_T together:  Latency = 0.29805999999999994 us and Energy = 0.6938796441600025 uJ
Operation softmax: Latency = 0.26605999999999996 us and Energy = 0.6882271641599983
attention_scores.shape
torch.Size([1, 12, 256, 256])
cap_mac_lat: 1.1 cap_mac_en: 71.07
debug cap_mac: xbar_count: 32.0 wl_count: 256 lat_operation_count: 256 en_operation_count: 98304.0 cap_mac_lat: 1.1 cap_mac_en: 71.07
qkt*v output: lat,en in us and uJ
2.601772 80.25807246528
Operation5: CAP-MAC Softmax output*V computation:  Latency = 0.2815999999999999 us and Energy = 6.986465280000001 uJ


attention_matrix.shape
torch.Size([1, 12, 256, 64])
Operation5.5: lat_only_write_mha = 0.032 us and en_only_write_mha = 0.0014131200000047685 uJ

after writing soft(qkt)*v in us and uJ
2.633772 80.25948558528


fet_mac_lat: 1.1 fet_mac_en: 74
debug fet_mac: xbar_count: 1152.0 wl_count: 256 lat_operation_count: 256 en_operation_count: 294912.0 fet_mac_lat: 1.1 fet_mac_en: 74
MHA output lat,en results in us and uJ
2.9153719999999996 102.08297358528
Operation6: FET MAC: Multiplication with W_O:  Latency = 0.2815999999999999 us and Energy = 21.823488 uJ


attention_matrix.shape
torch.Size([1, 256, 768])
After writing MHA output to the FET: lat,en in us and uJ
4.195371999999999 102.08400270528
add_dram:x with mha output:  lat,en in us and uJ
4.897132 102.13979022528


Operation norm ADD: lat_only_norm_add: 0.7017600000000003 us and en_only_norm_add = 0.05578751999999583 uJ

layernorm output1 shape:
torch.Size([1, 256, 768])
after writing the output of the x+mha_output in the fet: lat,en in us and uJ
4.897132 102.13979022528


ff1 layer output shape:
torch.Size([1, 256, 3072])
fet_mac_lat: 1.1 fet_mac_en: 74
batch: 1, seq_len: 256, ff_hidden_dim: 3072, d_model: 768
debug fet_mac: xbar_count: 4608.0 wl_count: 256 lat_operation_count: 256 en_operation_count: 1179648.0 fet_mac_lat: 1.1 fet_mac_en: 74
ff1 mac output: lat,en in us and uJ
5.178732 189.43374222527999
Operation7: FET MAC FF1:  Latency = 0.28160000000000035 us and Energy = 87.29395199999999 uJ
ff1 mac output FET write: lat,en in us and uJ
6.458732 189.43785870527998


ff_relu shape:
torch.Size([1, 256, 3072])
fet_mac_lat: 1.1 fet_mac_en: 74
batch: 1, seq_len: 256, ff_hidden_dim: 3072, d_model: 768
debug fet_mac: xbar_count: 4608.0 wl_count: 256 lat_operation_count: 256 en_operation_count: 1179648.0 fet_mac_lat: 1.1 fet_mac_en: 74
FF2 mac operation output: lat,en in us and uJ
6.740332 276.73181070527994
Operation8: FET MAC FF2:  Latency = 0.28160000000000035 us and Energy = 87.29395199999998 uJ
ff2 mac output FET write: lat,en in us and uJ
8.020332 276.73283982527994


ff_out.shape
torch.Size([1, 256, 768])
torch.Size([1, 256, 768])
Operation norm ADD2: latency = 0.7017600000000003 us and energy = 0.7017600000000003 uJ

Encoder layer output: add_dram x+ff2 : lat,en in us and uJ
8.722092 276.7886273452799
Encoder output write FET: lat,en in us and uJ
8.722092 276.7886273452799


lat_before_mha,lat_after_mha,lat_after_encoder in us and uJ
0.89568 2.633772 8.722092
en_before_mha,en_after_mha,en_after_encoder in us and uJ
0.11655890111999997 80.25948558528 276.7886273452799


end_to_end_latency is 0.094812624 ms and end_to_end_energy is 3.320181380231039 mJ
