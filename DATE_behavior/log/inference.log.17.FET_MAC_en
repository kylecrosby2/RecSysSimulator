x.shape
torch.Size([1, 512, 768])
input_ids have 23 tokens
Operation-1: we and pe write: before add_cap to get xpe: latency in ms: energy in mJ:
3.7094400000000003 0.00252626112
Operation0: we+pe = xpe: output: latency in ms, energy in mJ
0.7343999999999996 0.22806527999999998
w_q.weight
Parameter containing:
tensor([[-0.0347, -0.0180, -0.0062,  ..., -0.0333,  0.0106, -0.0210],
        [-0.0339,  0.0014,  0.0191,  ...,  0.0199, -0.0263, -0.0135],
        [ 0.0293,  0.0144,  0.0176,  ..., -0.0103,  0.0334, -0.0190],
        ...,
        [ 0.0067, -0.0161,  0.0052,  ..., -0.0208, -0.0251,  0.0085],
        [ 0.0077,  0.0221,  0.0204,  ..., -0.0149,  0.0129, -0.0235],
        [ 0.0014,  0.0108,  0.0070,  ...,  0.0155, -0.0134, -0.0189]],
       requires_grad=True)
w_q.shape
torch.Size([768, 768])
fet_mac_lat: 1.1 fet_mac_en: 74
debug fet_mac: xbar_count: 1152.0 wl_count: 512 lat_operation_count: 512 en_operation_count: 589824.0 fet_mac_lat: 1.1 fet_mac_en: 74
fet_mac_lat: 1.1 fet_mac_en: 74
debug fet_mac: xbar_count: 1152.0 wl_count: 512 lat_operation_count: 512 en_operation_count: 589824.0 fet_mac_lat: 1.1 fet_mac_en: 74
fet_mac_lat: 1.1 fet_mac_en: 74
debug fet_mac: xbar_count: 1152.0 wl_count: 512 lat_operation_count: 512 en_operation_count: 589824.0 fet_mac_lat: 1.1 fet_mac_en: 74
lat,en: fet mac output compute q,k,v
6.133439999999999 131.17151954112
Operation1: Latency and energy from MUL of X and W_Q: 
0.5631999999999998 us and 43.646976 uJ
query.shape
torch.Size([1, 512, 768])
query.shape
torch.Size([1, 12, 512, 64])
lat,en: after cap_bl_to_sa_wr_rm: q,k,v write
8.437439999999999 131.17999826111998
Operation2: cap_write q,k,v: 
Latency = 2.303999999999999 us and Energy = 0.008478719999983907 uJ
cap_mac_lat: 1.1 cap_mac_en: 71.07
debug cap_mac: xbar_count: 64.0 wl_count: 512 lat_operation_count: 512 en_operation_count: 393216.0 cap_mac_lat: 1.1 cap_mac_en: 71.07
lat,en: after qkt mac computation
9.000639999999999 159.12585938111997
Operation3: cap mac QK_T: 
Latency = 0.5632000000000007 us and Energy = 27.94586111999999 uJ
attention_scores.shape
torch.Size([1, 12, 512, 512])
qk_t write output: lat,en in us and uJ
15.144639999999999 159.14846930111995
softmax output: lat,en in us and uJ
1281.30016 159.62447299775997


Operation4: Softmax and writing QK_T together:  
Latency = 1272.29952 us and Energy = 0.49861361664000153 uJ
attention_scores.shape
torch.Size([1, 12, 512, 512])
cap_mac_lat: 1.1 cap_mac_en: 71.07
debug cap_mac: xbar_count: 64.0 wl_count: 512 lat_operation_count: 512 en_operation_count: 393216.0 cap_mac_lat: 1.1 cap_mac_en: 71.07
qkt*v output: lat,en in us and uJ
1281.8633599999998 187.57033411775998
Operation5: CAP-MAC Softmax output*V computation:  
Latency = 0.5631999999999534 us and Energy = 27.945861120000004 uJ


attention_matrix.shape
torch.Size([1, 12, 512, 64])
after writing soft(qkt)*v in us and uJ
1281.89536 187.57316035775997


fet_mac_lat: 1.1 fet_mac_en: 74
debug fet_mac: xbar_count: 1152.0 wl_count: 512 lat_operation_count: 512 en_operation_count: 589824.0 fet_mac_lat: 1.1 fet_mac_en: 74
MHA output lat,en results in us and uJ
1282.4585599999998 231.22013635775997
Operation6: FET MAC: Multiplication with W_O:  
Latency = 0.5631999999999534 us and Energy = 43.646976 uJ


attention_matrix.shape
torch.Size([1, 512, 768])
After writing MHA output to the FET: lat,en in us and uJ
1283.7385599999998 231.22219459776
add_dram:x with mha output:  lat,en in us and uJ
1284.44032 231.33376963775999


layernorm output1 shape:
torch.Size([1, 512, 768])
after writing the output of the x+mha_output in the fet: lat,en in us and uJ
1284.44032 231.33376963775999


ff1 layer output shape:
torch.Size([1, 512, 3072])
fet_mac_lat: 1.1 fet_mac_en: 74
batch: 1, seq_len: 512, ff_hidden_dim: 3072, d_model: 768
debug fet_mac: xbar_count: 4608.0 wl_count: 512 lat_operation_count: 512 en_operation_count: 2359296.0 fet_mac_lat: 1.1 fet_mac_en: 74
ff1 mac output: lat,en in us and uJ
1285.0035199999998 405.92167363775997
Operation7: FET MAC FF1:  
Latency = 0.5631999999999534 us and Energy = 174.587904 uJ
ff1 mac output FET write: lat,en in us and uJ
1286.2835199999997 405.92990659775995


ff_relu shape:
torch.Size([1, 512, 3072])
fet_mac_lat: 1.1 fet_mac_en: 74
batch: 1, seq_len: 512, ff_hidden_dim: 3072, d_model: 768
debug fet_mac: xbar_count: 4608.0 wl_count: 512 lat_operation_count: 512 en_operation_count: 2359296.0 fet_mac_lat: 1.1 fet_mac_en: 74
FF2 mac operation output: lat,en in us and uJ
1286.8467199999998 580.51781059776
Operation8: FET MAC FF2:  
Latency = 0.5631999999999534 us and Energy = 174.587904 uJ
ff2 mac output FET write: lat,en in us and uJ
1288.1267199999998 580.51986883776


ff_out.shape
torch.Size([1, 512, 768])
torch.Size([1, 512, 768])
Encoder layer output: add_dram x+ff2 : lat,en in us and uJ
1288.8284799999997 580.6314438777599
Encoder output write FET: lat,en in us and uJ
1288.8284799999997 580.6314438777599


lat_before_mha,lat_after_mha,lat_after_encoder in us and uJ
4.44384 1281.89536 1288.8284799999997
en_before_mha,en_after_mha,en_after_encoder in us and uJ
0.23059154112 187.57316035775997 580.6314438777599


end_to_end_latency is 15.417059519999995 ms and end_to_end_energy is 6.965040819580798 mJ
